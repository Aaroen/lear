本文件夹主要实现西瓜书部分函数:

《机器学习》西瓜书的主要章节和其中涉及的可实践实现的算法/功能：

**整体章节梳理**
1.  **第1章 绪论:** 概念为主，无特定算法实现。
2.  **第2章 模型评估与选择:**
    *   **可实践:** 实现不同的**评估指标**（如精度、错误率、查准率、查全率、F1 值、AUC）、**交叉验证**（k 折交叉验证、留一法）、混淆矩阵计算。这是任何机器学习项目的基础。
3.  **第3章 线性模型:**
    *   **可实践:**
        *   **线性回归 (Linear Regression):** 实现基于最小二乘法的参数求解（正规方程或梯度下降）。
        *   **对数几率回归 (Logistic Regression):** 实现基于梯度下降或牛顿法的参数优化。
        *   **线性判别分析 (LDA):** 实现 LDA 的投影计算和分类。
4.  **第4章 决策树:**
    *   **可实践:**
        *   **ID3 决策树:** 实现基于信息增益的节点划分。
        *   **C4.5 决策树:** 实现基于增益率的节点划分（处理 ID3 的偏好问题）。
        *   **CART 决策树:** 实现基于基尼指数的分类树或平方误差最小化的回归树。
        *   **剪枝处理:** 实现预剪枝或后剪枝策略。
5.  **第5章 神经网络:**
    *   **可实践:**
        *   **标准 BP (Backpropagation) 算法:** 实现一个简单的前馈神经网络（多层感知机 MLP），并使用 BP 算法进行训练。这是理解深度学习的基础。
        *   **累积 BP 算法:** 实现另一种形式的 BP 训练。
6.  **第6章 支持向量机 (SVM):**
    *   **可实践:** 理解 SVM 原理比较关键。从零实现（特别是涉及 SMO 算法）复杂度较高，但可以：
        *   实现一个**简单的线性 SVM**（硬间隔或软间隔）的对偶问题求解（可以使用简单的优化库辅助）。
        *   理解**核函数**的作用，尝试使用不同的核函数（如高斯核）并观察效果（通常借助库实现）。
7.  **第7章 贝叶斯分类器:**
    *   **可实践:**
        *   **朴素贝叶斯分类器 (Naive Bayes):** 实现基于贝叶斯定理和特征条件独立假设的分类器（处理离散和连续属性，拉普拉斯平滑）。非常经典且容易实现。
8.  **第8章 集成学习:**
    *   **可实践:**
        *   **AdaBoost 算法:** 实现 AdaBoost 算法，理解其样本权重调整和基学习器组合的过程。
        *   **Bagging:** 实现 Bagging 过程（自助采样、训练基学习器、投票/平均）。
        *   **(可选) 随机森林 (Random Forest):** 在 Bagging 基础上加入随机属性选择（通常直接使用库，但理解原理重要）。
9.  **第9章 聚类:**
    *   **可实践:**
        *   **K-Means 算法:** 实现 K-Means 聚类过程（随机初始化中心、分配样本、更新中心）。
        *   **DBSCAN 算法:** 实现基于密度的聚类算法。
        *   **AGNES 算法:** 实现层次聚类中的凝聚算法。
10. **第10章 降维与度量学习:**
    *   **可实践:**
        *   **主成分分析 (PCA):** 实现 PCA 算法，计算协方差矩阵、特征值分解、选择主成分进行降维。
        *   **(可选) 核化 PCA (KPCA):** 理解核技巧在 PCA 中的应用。
11. **第11章 特征选择与稀疏学习:** 概念和策略为主，特定算法如 L1 正则化在线性模型中已涉及。可以实践过滤式、包裹式、嵌入式的特征选择思想。
12. **第12章 计算学习理论:** 理论为主。
13. **第13章 半监督学习:**
    *   **可实践:** 实现简单的**自训练 (Self-training)** 或**协同训练 (Co-training)** 算法。
14. **第14章 概率图模型:** HMM, MRF, LDA 等模型实现复杂度较高，但可以：
    *   **可实践 (进阶):** 实现隐马尔可夫模型 (HMM) 的前向、后向算法或维特比算法。
15. **第15章 规则学习:** 概念为主。
16. **第16章 强化学习:**
    *   **可实践:** 实现简单的**Q-Learning** 或 **Sarsa** 算法在一个简单的格子世界 (Grid World) 环境中。


**学习法则:**

1.  **对照书籍:** 在实现代码时，务必对照西瓜书上的公式和算法伪代码。
2.  **先理解再动手:** 不要急于复制代码，先确保理解了算法的每一步。
3.  **从简单开始:** 可以先实现一个简化版的算法，再逐步完善。
4.  **使用 NumPy:** 尽量使用 NumPy 进行数值计算，避免使用大量原生 Python 循环，以提高效率并熟悉 NumPy 的使用。
5.  **测试与验证:** 使用简单的数据集（甚至手写几个样本）来测试你的实现是否正确。可以将结果与 Scikit-learn 等库的实现进行对比验证。
6.  **阅读他人代码:** 参考 GitHub 上的实现，学习不同的代码风格和实现技巧，但要保持批判性思维。



**核心实践部分**

以下是针对之前筛选出的 8 个核心部分的详细信息、实现思路和方法：

**1. 评估指标与交叉验证 (第 2 章)**

*   **核心目标:** 准确评估模型的泛化性能，避免过拟合或欠拟合的误判，并选择最优模型。
*   **关键概念:**
    *   混淆矩阵 (Confusion Matrix): TP, TN, FP, FN
    *   评估指标: 准确率 (Accuracy), 错误率 (Error Rate), 查准率 (Precision), 查全率 (Recall), F1 值, ROC 曲线, AUC 值。
    *   验证策略: K 折交叉验证 (k-fold Cross Validation)。
*   **实现思路与步骤:**
    1.  **混淆矩阵:**
        *   输入: 真实标签列表 `y_true`, 预测标签列表 `y_pred`。
        *   输出: 2x2 的 NumPy 数组表示混淆矩阵。
        *   方法: 遍历 `y_true` 和 `y_pred`，根据 (true, pred) 对累加到矩阵的对应位置 (如 (1,1) 是 TP, (0,0) 是 TN, (0,1) 是 FP, (1,0) 是 FN，假设 1 是正类, 0 是负类)。
    2.  **基础评估指标 (基于混淆矩阵):**
        *   输入: 混淆矩阵。
        *   输出: Accuracy, Precision, Recall, F1。
        *   方法: 根据各自的公式 (Accuracy = (TP+TN)/Total, Precision = TP/(TP+FP), Recall = TP/(TP+FN), F1 = 2 * Precision * Recall / (Precision + Recall)) 计算。注意处理分母为零的情况（返回 0 或 NaN，或加小 epsilon）。
    3.  **K 折交叉验证:**
        *   输入: 完整数据集 X, y, 折数 k。
        *   输出: k 次验证的评估指标列表（或平均值）。
        *   方法:
            *   将数据集索引随机打乱。
            *   将打乱后的索引平均分成 k 份。
            *   进行 k 次循环 (i 从 0 到 k-1):
                *   取第 i 份作为验证集 (validation set)。
                *   其余 k-1 份合并作为训练集 (training set)。
                *   在训练集上训练模型。
                *   在验证集上评估模型，记录指标。
            *   计算 k 次指标的平均值和标准差。
*   **注意事项:**
    *   确保指标计算与类别定义一致（哪个是正类）。
    *   交叉验证前务必打乱数据，否则可能因数据顺序导致偏差。
    *   分层 K 折交叉验证 (Stratified K-Fold) 在类别不平衡时更优，可以考虑实现（确保每折中类别比例与原始数据相似）。
*   **验证方法:** 使用简单已知结果的数据集手动计算验证；使用 `sklearn.metrics` 中的 `confusion_matrix`, `accuracy_score`, `precision_score`, `recall_score`, `f1_score` 和 `sklearn.model_selection.KFold` 或 `StratifiedKFold` 对比结果。

**2. 逻辑回归 (Logistic Regression) (第 3 章)**

*   **核心目标:** 实现一个用于二分类问题的线性模型，输出属于正类的概率。
*   **关键概念:**
    *   Sigmoid 函数: `σ(z) = 1 / (1 + exp(-z))`，将线性输出映射到 (0, 1) 区间。
    *   线性边界: `z = w^T * x + b` (或将 b 看作 w 的一部分，x 增加一维常数 1)。
    *   损失函数: 对数损失 (Log Loss) 或交叉熵损失。
    *   优化算法: 梯度下降 (Gradient Descent)。
*   **实现思路与步骤:**
    1.  **初始化:** 初始化权重向量 `w` (通常为 0 或小的随机数) 和偏置 `b` (通常为 0)。
    2.  **预测函数:**
        *   输入: 特征数据 `X`, 权重 `w`, 偏置 `b`。
        *   输出: 预测概率 `p` (样本属于正类的概率)。
        *   方法: 计算 `z = X * w + b` (NumPy 矩阵乘法)，然后 `p = sigmoid(z)`。
    3.  **损失函数:**
        *   输入: 真实标签 `y` (0 或 1), 预测概率 `p`。
        *   输出: 平均对数损失。
        *   方法: `loss = -mean(y * log(p) + (1 - y) * log(1 - p))`。注意数值稳定性，避免 `log(0)`。
    4.  **梯度计算:**
        *   输入: 特征 `X`, 真实标签 `y`, 预测概率 `p`。
        *   输出: 损失函数对 `w` 和 `b` 的梯度 `dw`, `db`。
        *   方法: `dz = p - y`, `dw = (X^T * dz) / m` (m 是样本数), `db = mean(dz)`。
    5.  **梯度下降训练:**
        *   输入: 训练数据 `X_train`, `y_train`, 学习率 `alpha`, 迭代次数 `epochs`。
        *   输出: 训练好的 `w` 和 `b`。
        *   方法:
            *   循环 `epochs` 次:
                *   调用预测函数计算 `p`。
                *   (可选) 调用损失函数计算并记录 `loss`。
                *   调用梯度计算函数得到 `dw`, `db`。
                *   更新参数: `w = w - alpha * dw`, `b = b - alpha * db`。
*   **注意事项:**
    *   特征缩放 (Standardization 或 Normalization) 对梯度下降收敛速度很重要。
    *   学习率 `alpha` 的选择很关键，太大可能不收敛，太小收敛慢。
    *   向量化实现（使用 NumPy）比用 for 循环效率高得多。
*   **验证方法:** 使用简单的线性可分数据集；与 `sklearn.linear_model.LogisticRegression` 对比权重和预测结果。

**3. 决策树 (ID3 或 CART) (第 4 章)**

*   **核心目标:** 构建一个树形结构，通过一系列基于特征的判断来进行分类或回归。这里以 ID3 (分类) 为例。
*   **关键概念:**
    *   信息熵 (Information Entropy): 度量样本集合纯度。`Ent(D) = - sum(p_k * log2(p_k))` (k 是类别)。
    *   信息增益 (Information Gain): 特征 A 对数据集 D 的信息增益 `Gain(D, A) = Ent(D) - sum(|D^v| / |D| * Ent(D^v))` (v 是特征 A 的取值)。
    *   递归构建。
*   **实现思路与步骤:**
    1.  **熵计算函数:**
        *   输入: 数据集 D (或标签列表 y)。
        *   输出: 数据集 D 的信息熵。
        *   方法: 计算每个类别的比例 `p_k`，代入熵公式。
    2.  **信息增益计算函数:**
        *   输入: 数据集 D, 特征索引 `feature_idx`。
        *   输出: 特征 `feature_idx` 对数据集 D 的信息增益。
        *   方法:
            *   计算 `Ent(D)`。
            *   对特征 `feature_idx` 的每个可能取值 `v`：
                *   筛选出子集 `D^v`。
                *   计算 `Ent(D^v)`。
            *   根据信息增益公式计算 `Gain(D, feature_idx)`。
    3.  **选择最佳划分特征函数:**
        *   输入: 数据集 D, 可用特征列表 `feature_list`。
        *   输出: 信息增益最大的特征索引。
        *   方法: 遍历 `feature_list`，调用信息增益计算函数，返回增益最大的特征。
    4.  **递归构建树函数 `TreeGenerate(D, feature_list)`:**
        *   输入: 当前节点的数据集 D, 可用特征列表 `feature_list`。
        *   输出: 构建好的子树 (可以是字典或自定义类对象)。
        *   方法 (核心递归逻辑):
            *   **基本情况 (停止条件):**
                *   如果 D 中所有样本属于同一类别 C，返回 C 作为叶节点。
                *   如果 `feature_list` 为空，或 D 中所有样本在 `feature_list` 上取值相同，无法划分，返回 D 中样本数最多的类别作为叶节点。
            *   **递归步骤:**
                *   调用 `选择最佳划分特征函数` 找到最佳特征 `best_feature`。
                *   创建以 `best_feature` 为根的节点。
                *   从 `feature_list` 中移除 `best_feature`。
                *   对 `best_feature` 的每个可能取值 `v`：
                    *   筛选出子集 `D^v`。
                    *   如果 `D^v` 为空，添加一个叶节点，类别为 D 中样本最多的类。
                    *   否则，递归调用 `TreeGenerate(D^v, feature_list)`，将返回的子树作为当前节点的分支。
                *   返回构建好的子树。
*   **注意事项:**
    *   ID3 倾向于选择取值多的特征，C4.5 (使用增益率) 或 CART (使用基尼指数) 可以缓解。
    *   需要处理连续值特征（如二分法找最佳分裂点）和缺失值（赋予权重或选择特定策略）。从零实现可以先只考虑离散特征且无缺失值。
    *   容易过拟合，需要剪枝（预剪枝或后剪枝），从零实现可以先不考虑剪枝。
*   **验证方法:** 使用书中的西瓜数据集 2.0 或 3.0 手动模拟或运行代码；与 `sklearn.tree.DecisionTreeClassifier(criterion='entropy')` 对比树结构和预测结果。

**4. 朴素贝叶斯分类器 (Naive Bayes) (第 7 章)**

*   **核心目标:** 基于贝叶斯定理和特征条件独立假设进行分类。
*   **关键概念:**
    *   贝叶斯定理: `P(c|x) = P(x|c) * P(c) / P(x)`。
    *   特征条件独立假设: `P(x|c) = product(P(x_i|c))` (x_i 是第 i 个特征)。
    *   先验概率 (Prior): `P(c)`。
    *   类条件概率 (Likelihood): `P(x_i|c)`。
    *   拉普拉斯平滑 (Laplacian Smoothing): 避免概率为 0。
*   **实现思路与步骤:**
    1.  **训练阶段 (计算概率):**
        *   输入: 训练数据 `X_train`, `y_train`。
        *   输出: 先验概率 `P(c)` 和类条件概率 `P(x_i|c)`。
        *   方法:
            *   计算每个类别 `c` 的先验概率 `P(c) = count(c) / total_samples`。
            *   对每个类别 `c` 和每个特征 `i`：
                *   计算特征 `i` 的每个可能取值 `v` 在类别 `c` 下出现的条件概率 `P(x_i=v|c) = (count(x_i=v and y=c) + lambda) / (count(y=c) + N_i * lambda)` (N_i 是特征 i 的可能取值数，lambda 是平滑参数，通常为 1)。
                *   (对于连续特征，通常假设高斯分布，计算每个类别下该特征的均值和方差)。
            *   存储所有计算出的概率。
    2.  **预测阶段:**
        *   输入: 新样本 `x_new`, 训练阶段计算出的概率。
        *   输出: 预测类别 `c_pred`。
        *   方法:
            *   对每个类别 `c`：
                *   计算后验概率得分 (通常忽略分母 P(x)): `score(c) = log(P(c)) + sum(log(P(x_new_i|c)))` (取对数避免下溢，连乘变连加)。注意使用训练阶段计算好的 `P(c)` 和 `P(x_new_i|c)` (如果 `x_new_i` 的取值在训练集未出现，平滑会处理)。
            *   选择 `score(c)` 最大的类别作为预测结果 `c_pred`。
*   **注意事项:**
    *   拉普拉斯平滑是必需的，防止零概率问题。
    *   需要区分处理离散特征和连续特征（如高斯朴素贝叶斯）。
    *   特征条件独立假设在现实中往往不成立，但朴素贝叶斯在很多场景下效果依然不错。
*   **验证方法:** 使用简单的文本分类数据集（如垃圾邮件分类）；与 `sklearn.naive_bayes.CategoricalNB` (离散) 或 `GaussianNB` (连续) 对比结果。

**5. K-Means 聚类 (第 9 章)**

*   **核心目标:** 将数据点划分为 K 个簇 (cluster)，使得簇内点距离近，簇间点距离远。
*   **关键概念:**
    *   簇中心 (Centroid)。
    *   距离度量 (通常是欧氏距离)。
    *   迭代优化。
*   **实现思路与步骤:**
    1.  **初始化:** 随机选择 K 个数据点作为初始簇中心 `centroids`。
    2.  **迭代过程:**
        *   循环直到簇中心不再（或很少）变化 或 达到最大迭代次数:
            *   **分配步骤 (Assignment):**
                *   创建一个列表 `clusters` 用于存储每个簇包含的数据点索引。
                *   对每个数据点 `x_i`：
                    *   计算 `x_i` 到 K 个簇中心 `centroids` 的距离。
                    *   将 `x_i` 分配给距离最近的那个簇 `j`，将 `i` 添加到 `clusters[j]`。
            *   **更新步骤 (Update):**
                *   对每个簇 `j` (从 0 到 K-1):
                    *   如果簇 `j` 不为空，计算簇内所有点的均值，更新 `centroids[j]` 为该均值。
                    *   (如果簇为空，可以保持原中心不变，或重新随机初始化，或移除该簇 - 策略可选)。
*   **注意事项:**
    *   K 值的选择需要先验知识或通过方法（如肘部法则）确定。
    *   对初始簇中心敏感，可能陷入局部最优。可以多次运行，选择最优结果（如总簇内平方和最小）。
    *   对异常值敏感。
*   **验证方法:** 使用二维或三维点集进行可视化；与 `sklearn.cluster.KMeans` 对比簇分配结果和簇中心位置。

**6. 主成分分析 (PCA) (第 10 章)**

*   **核心目标:** 通过线性变换将高维数据投影到低维空间，同时最大化保留原始数据的方差。
*   **关键概念:**
    *   方差与协方差。
    *   协方差矩阵。
    *   特征值与特征向量。
*   **实现思路与步骤:**
    1.  **数据中心化/标准化:** 对原始数据 `X` 的每一列（特征）减去其均值 (中心化)。通常也进行标准化（除以标准差）。令处理后的数据为 `X_norm`。
    2.  **计算协方差矩阵:** `Cov = (X_norm^T * X_norm) / (m - 1)` (m 是样本数)。
    3.  **计算特征值和特征向量:** 对协方差矩阵 `Cov` 进行特征值分解，得到特征值 `eigenvalues` 和对应的特征向量 `eigenvectors`。
    4.  **选择主成分:** 将特征值从大到小排序，选择前 `k` 个最大特征值对应的特征向量，组成投影矩阵 `W` (形状为 d x k, d 是原始维度)。
    5.  **数据投影:** 将中心化/标准化后的数据 `X_norm` 投影到选定的主成分上: `X_pca = X_norm * W` (得到 k 维数据)。
*   **注意事项:**
    *   PCA 对数据尺度敏感，预处理（标准化）通常是必需的。
    *   选择的 `k` 值决定了降维后的维度，可以根据“解释方差比”（explained variance ratio）来选择，即前 k 个特征值的和占总特征值和的比例。
*   **验证方法:** 使用鸢尾花 (Iris) 等经典数据集进行降维（如降到 2 维）并可视化；与 `sklearn.decomposition.PCA` 对比降维结果和解释方差比。

**7. BP 算法 (简单 MLP) (第 5 章)**

*   **核心目标:** 实现一个包含至少一个隐藏层的前馈神经网络，并使用反向传播算法训练权重。
*   **关键概念:**
    *   网络结构: 输入层、隐藏层（可多个）、输出层。
    *   激活函数 (Activation Function): 如 Sigmoid, Tanh, ReLU。
    *   前向传播 (Forward Propagation): 从输入到输出计算网络预测值。
    *   损失函数 (Loss Function): 如均方误差 (MSE) 用于回归，交叉熵 (Cross-Entropy) 用于分类。
    *   反向传播 (Backpropagation): 从输出层反向计算误差对各层权重的梯度（链式法则）。
    *   梯度下降: 使用计算出的梯度更新权重。
*   **实现思路与步骤 (以单隐层网络，Sigmoid 激活为例):**
    1.  **初始化:** 初始化输入层到隐藏层的权重 `W1`, 偏置 `b1`；隐藏层到输出层的权重 `W2`, 偏置 `b2` (通常使用小的随机数)。
    2.  **前向传播函数:**
        *   输入: 样本 `x`。
        *   输出: 隐藏层激活值 `H`, 输出层预测值 `y_pred`。
        *   方法: `Z1 = x * W1 + b1`, `H = sigmoid(Z1)`, `Z2 = H * W2 + b2`, `y_pred = sigmoid(Z2)` (如果是回归任务，输出层可能不用 sigmoid)。
    3.  **损失函数 (MSE):** `loss = mean((y_true - y_pred)^2)`。
    4.  **反向传播计算梯度:**
        *   输入: `x`, `H`, `y_pred`, `y_true`, `W2`。
        *   输出: `dW1`, `db1`, `dW2`, `db2`。
        *   方法 (核心，应用链式法则):
            *   计算输出层误差: `delta_output = (y_pred - y_true) * sigmoid_derivative(Z2)` (注意 sigmoid 导数是 `sigmoid(z)*(1-sigmoid(z))`)。
            *   计算对 W2, b2 的梯度: `dW2 = H^T * delta_output`, `db2 = mean(delta_output, axis=0)`。
            *   计算隐藏层误差: `delta_hidden = delta_output * W2^T * sigmoid_derivative(Z1)`。
            *   计算对 W1, b1 的梯度: `dW1 = x^T * delta_hidden`, `db1 = mean(delta_hidden, axis=0)`。
    5.  **训练循环:**
        *   输入: 训练数据 `X_train`, `y_train`, 学习率 `alpha`, 迭代次数 `epochs`。
        *   输出: 训练好的 `W1, b1, W2, b2`。
        *   方法:
            *   循环 `epochs` 次:
                *   (可选) 对小批量 (mini-batch) 数据进行处理。
                *   调用前向传播计算 `H`, `y_pred`。
                *   计算损失 `loss`。
                *   调用反向传播计算梯度 `dW1, db1, dW2, db2`。
                *   更新权重: `W1 -= alpha * dW1`, `b1 -= alpha * db1`, etc.
*   **注意事项:**
    *   权重初始化很重要，避免对称性和梯度消失/爆炸。
    *   激活函数的选择（ReLU 通常优于 Sigmoid）。
    *   学习率调整、优化器选择（Adam 等）可以提升效果，但初学可用朴素梯度下降。
    *   理解矩阵维度匹配至关重要。
    *   这是手动实现中最复杂的部分之一，需要耐心调试。
*   **验证方法:** 使用 XOR 数据集或简单的分类/回归任务；监控损失函数是否下降；（高级）数值梯度检验对比反向传播计算的梯度。

**8. AdaBoost (第 8 章)**

*   **核心目标:** 实现一种 Boosting 集成学习算法，通过迭代训练一系列弱学习器（通常是决策树桩 Decision Stump），并根据错误率调整样本权重和学习器权重。
*   **关键概念:**
    *   弱学习器 (Weak Learner): 性能略优于随机猜测的模型。
    *   样本权重 (Sample Weights): 每轮迭代中赋予每个样本不同的重要性。
    *   学习器权重 (Learner Weight / Alpha): 每个弱学习器在最终投票中的重要性。
    *   加权错误率 (Weighted Error)。
*   **实现思路与步骤:**
    1.  **初始化:** 初始化样本权重 `D_1` 为均匀分布 `1/m` (m 是样本数)。
    2.  **迭代训练 (t = 1 to T):**
        *   **训练弱学习器:** 使用带有样本权重 `D_t` 的训练数据训练一个弱学习器 `h_t(x)` (例如，找到最佳分裂点的决策树桩)。
        *   **计算加权错误率:** `epsilon_t = sum(D_t[i] * I(h_t(x_i) != y_i))` (I 是指示函数，当条件为真时为 1，否则为 0)。
        *   **计算学习器权重:** `alpha_t = 0.5 * log((1 - epsilon_t) / epsilon_t)`。注意处理 `epsilon_t` 为 0 或 0.5 的情况。
        *   **更新样本权重:** `D_{t+1}[i] = (D_t[i] / Z_t) * exp(-alpha_t * y_i * h_t(x_i))` (假设 `y` 和 `h_t` 输出为 +1/-1；Z_t 是归一化因子，使 `sum(D_{t+1}) = 1`)。这个公式会增加被 `h_t` 错误分类的样本的权重。
        *   存储 `h_t` 和 `alpha_t`。
    3.  **最终预测:**
        *   输入: 新样本 `x_new`。
        *   输出: 最终预测结果。
        *   方法: `H(x_new) = sign(sum(alpha_t * h_t(x_new)))` (对所有 T 个弱学习器的预测进行加权投票)。
*   **注意事项:**
    *   弱学习器的选择，决策树桩是经典选择。实现一个简单的决策树桩函数作为子模块。
    *   需要正确处理标签格式 (+1/-1 通常更方便)。
    *   数值稳定性，特别是 `alpha_t` 的计算和权重更新。
*   **验证方法:** 使用简单的二分类数据集；观察样本权重在迭代过程中的变化；与 `sklearn.ensemble.AdaBoostClassifier` (使用 DecisionTreeClassifier(max_depth=1) 作为基学习器) 对比结果。

**核心思路:**

1.  **理解先行:** 在写代码前，务必彻底理解算法的理论、公式推导和伪代码。
2.  **模块化:** 将算法分解成小的、可管理的函数（如熵计算、梯度计算、距离计算、弱学习器训练等）。
3.  **NumPy 主导:** 尽可能使用 NumPy 进行向量和矩阵运算，以提高效率和代码简洁性。
4.  **数据流清晰:** 明确每个函数的输入和输出。
5.  **简单起步:** 先实现算法的核心逻辑，处理最简单的情况（如离散特征、无缺失值、二分类），然后再考虑扩展和优化。
6.  **验证迭代:** 使用简单数据和/或与成熟库（如 Scikit-learn）对比结果，不断调试和完善你的实现。
